{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize  your RAG Data - EDA for Retrieval-Augmented Generation\n",
    "## How to use UMAP dimensionality reduction for Embeddings to show  Questions, Answers and their relationships to source documents with OpenAI, Langchain and ChromaDB\n",
    "- This notebook is part of an article to-be-published. \n",
    "- You can skip all preparation steps and **go directly to the visualizations section if you only want to check out the visualization** of prepared data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-openai chromadb renumics-spotlight\n",
    "# %env OPENAI_API_KEY=<your-api-key> # uncomment and set your OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for caching expensive LMM Calls\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "import hashlib\n",
    "\n",
    "\n",
    "def write_dict_to_file(data_dict, filename):\n",
    "    \"\"\"write a dictionary as a json line to a file - allowing for appending\"\"\"\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(json.dumps(data_dict) + \"\\n\")\n",
    "\n",
    "\n",
    "def read_dicts_from_file(filename):\n",
    "    \"\"\"Read a json line file as a generator of dictionaries - allowing to load multiple dictionaries as list.\"\"\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "\n",
    "\n",
    "def add_cached_column_from_file(df, file_name, merge_on, column):\n",
    "    \"\"\"Read a file with cached list of dicts data write it to a dataframe.\"\"\"\n",
    "\n",
    "    if Path(file_name).exists():\n",
    "\n",
    "        cached_answer_correctness = (\n",
    "            pd.DataFrame(list(read_dicts_from_file(file_name)))\n",
    "            .drop_duplicates(\n",
    "                subset=[merge_on],\n",
    "            )[[column, merge_on]]\n",
    "            .dropna()\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        return df.merge(\n",
    "            cached_answer_correctness,\n",
    "            on=merge_on,\n",
    "            how=\"left\",\n",
    "        ).reset_index(drop=True)\n",
    "    else:\n",
    "        return df.insert(0, column, None)\n",
    "\n",
    "\n",
    "def stable_hash_meta(metadata) -> str:\n",
    "    \"\"\"Stable hash document based on its metadata.\"\"\"\n",
    "    return hashlib.sha1(json.dumps(metadata, sort_keys=True).encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare documents\n",
    "In the section we will create\n",
    "- `embeddings_model`: A OpenAI based model to create embeddings for documents\n",
    "- `docs`: A list of documents collected from the ./docs/ folder\n",
    "- `docs_vectorstore`: A vectorstore with the embeddings of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings model and vector store\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "docs_vectorstore = Chroma(\n",
    "    collection_name=\"docs_store\",\n",
    "    embedding_function=embeddings_model,\n",
    "    persist_directory=\"docs-db\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents with the LangChain document loader\n",
    "from langchain_community.document_loaders import BSHTMLLoader, DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    \"docs\",\n",
    "    glob=\"*.html\",\n",
    "    loader_cls=BSHTMLLoader,\n",
    "    loader_kwargs={\"open_encoding\": \"utf-8\"},\n",
    "    recursive=True,\n",
    "    show_progress=True,\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide documents into splits\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "splits_ids = [\n",
    "    {\"doc\": split, \"id\": stable_hash_meta(split.metadata)} for split in splits\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep splits that are not already in the vector store\n",
    "existing_ids = docs_vectorstore.get()[\"ids\"]\n",
    "new_splits_ids = [split for split in splits_ids if split[\"id\"] not in existing_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# add new splits to the vector store\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "if new_splits_ids:\n",
    "    docs_vectorstore.add_documents(\n",
    "        documents=[split[\"doc\"] for split in new_splits_ids],\n",
    "        ids=[split[\"id\"] for split in new_splits_ids],\n",
    "    )\n",
    "docs_vectorstore.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the LangChain\n",
    "In this section, we will use OpenAI and LanChain to prepare a\n",
    "- `rag_chain`: A LangChain that uses gpt-3.5 and the `docs_vectorestore` as retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create language model and retriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0.0)\n",
    "retriever = docs_vectorstore.as_retriever(search_kwargs={\"k\": 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a RAG prompt that includes the question and the source documents\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an assistant for question-answering tasks.\n",
    "Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\").\n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "ALWAYS return a \"SOURCES\" part in your answer.\n",
    "\n",
    "QUESTION: {question}\n",
    "=========\n",
    "{source_documents}\n",
    "=========\n",
    "FINAL ANSWER: \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a RAG chain that retrieves documents, generates an answer, and formats the answer\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"Content: {doc.page_content}\\nSource: {doc.metadata['source']}\" for doc in docs\n",
    "    )\n",
    "\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(\n",
    "        source_documents=(lambda x: format_docs(x[\"source_documents\"]))\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain = RunnableParallel(\n",
    "    {\n",
    "        \"source_documents\": retriever,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and ask evaluation questions\n",
    "In this section you will create a dataframe with the following columns:\n",
    "- `question`: the question you want to ask\n",
    "- `ground_truth`: the correct answer to the question\n",
    "- `question_by`: the methode used to create the question\n",
    "- `answer`: the answer of the RAG System\n",
    "- `source`: the ids of source documents retrieved by the RAG System\n",
    "- `embedding`: the embedding of the question\n",
    "\n",
    "You can skip this part and download the dataframe from: xy.com/df_questions_answers.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# load questios and answer pairs that were genrated using the chatGPT web interface\n",
    "qa_gpt4 = json.load(open(\"qa_gpt4.json\"))\n",
    "qa_gpt35 = json.load(open(\"qa_gpt35.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "import pickle\n",
    "\n",
    "# generate testset using ragas and gpt-3.5 for generation and gpt-4 for filtering\n",
    "if not Path(\"testset_generation_100a_gpt35-40.pickle\").exists():\n",
    "    generator = TestsetGenerator.from_default()\n",
    "    testset_100_gpt35_40 = generator.generate(docs, 100)\n",
    "\n",
    "    with open(\"testset_generation_100a_gpt35-40.pickle\", \"wb\") as f:\n",
    "        pickle.dump(testset_100_gpt35_40, f)\n",
    "\n",
    "else:\n",
    "    testset_100_gpt35_40 = pickle.load(\n",
    "        open(\"testset_generation_100a_gpt35-40.pickle\", \"rb\")\n",
    "    )\n",
    "\n",
    "# generate testset using ragas and gpt-3.5 for generation and for filtering\n",
    "if not Path(\"testset_generation_100a.pickle\").exists():\n",
    "    generator = TestsetGenerator.from_default(\n",
    "        openai_generator_llm=\"gpt-3.5-turbo-16k\", openai_filter_llm=\"gpt-3.5-turbo-16k\"\n",
    "    )\n",
    "    testset_100_gpt35 = generator.generate(docs, 100)\n",
    "\n",
    "    with open(\"testset_generation_100a.pickle\", \"wb\") as f:\n",
    "        pickle.dump(testset_100_gpt35, f)\n",
    "else:\n",
    "    testset_100_gpt35 = pickle.load(open(\"testset_generation_100a.pickle\", \"rb\"))\n",
    "\n",
    "# generate testset using ragas and gpt-3.5 for generation and for filtering\n",
    "if not Path(\"testset_generation_100b.pickle\").exists():\n",
    "    generator = TestsetGenerator.from_default(\n",
    "        openai_generator_llm=\"gpt-3.5-turbo-16k\", openai_filter_llm=\"gpt-4-turbo-16k\"\n",
    "    )\n",
    "    testset_100_gpt35_b = generator.generate(docs, 100)\n",
    "\n",
    "    with open(\"testset_generation_100b.pickle\", \"wb\") as f:\n",
    "        pickle.dump(testset_100_gpt35_b, f)\n",
    "else:\n",
    "    testset_100_gpt35_b = pickle.load(open(\"testset_generation_100b.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all questions in one list with dict with the keys question, ground_truth, question_by\n",
    "questions_all = [\n",
    "    {\"question\": qa[\"question\"], \"ground_truth\": qa[\"answer\"], \"question_by\": \"gpt4\"}\n",
    "    for qa in qa_gpt4\n",
    "]\n",
    "questions_all += [\n",
    "    {\"question\": qa[\"question\"], \"ground_truth\": qa[\"answer\"], \"question_by\": \"gpt35\"}\n",
    "    for qa in qa_gpt35\n",
    "]\n",
    "questions_all += [\n",
    "    {\n",
    "        \"question\": qa.question,\n",
    "        \"ground_truth\": qa.ground_truth,\n",
    "        \"question_by\": \"rags_gpt35_40\",\n",
    "    }\n",
    "    for qa in testset_100_gpt35_40.test_data\n",
    "]\n",
    "questions_all += [\n",
    "    {\n",
    "        \"question\": qa.question,\n",
    "        \"ground_truth\": qa.ground_truth,\n",
    "        \"question_by\": \"ragas_gpt35\",\n",
    "    }\n",
    "    for qa in testset_100_gpt35.test_data\n",
    "]\n",
    "questions_all += [\n",
    "    {\n",
    "        \"question\": qa.question,\n",
    "        \"ground_truth\": qa.ground_truth,\n",
    "        \"question_by\": \"ragas_gpt35\",\n",
    "    }\n",
    "    for qa in testset_100_gpt35_b.test_data\n",
    "]\n",
    "\n",
    "len(questions_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a dataframe with questions and ground truth answers\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_questions = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": [f\"Question {i}\" for i, _ in enumerate(questions_all)],\n",
    "        \"question\": [qa[\"question\"] for qa in questions_all],\n",
    "        \"ground_truth\": [qa[\"ground_truth\"] for qa in questions_all],\n",
    "        \"question_by\": [qa[\"question_by\"] for qa in questions_all],\n",
    "    }\n",
    ")\n",
    "# keep only the first question if questions are duplicated\n",
    "df_questions = df_questions.drop_duplicates(subset=[\"question\"])\n",
    "df_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract embeddings for all documents from the vector store and store them in a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "all_docs = docs_vectorstore.get(include=[\"metadatas\", \"documents\", \"embeddings\"])\n",
    "df_docs = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": [stable_hash_meta(metadata) for metadata in all_docs[\"metadatas\"]],\n",
    "        \"source\": [metadata.get(\"source\") for metadata in all_docs[\"metadatas\"]],\n",
    "        \"page\": [metadata.get(\"page\", -1) for metadata in all_docs[\"metadatas\"]],\n",
    "        \"document\": all_docs[\"documents\"],\n",
    "        \"embedding\": all_docs[\"embeddings\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the cached RAG answers and source_documents ids from a file - or create an empty column\n",
    "df_questions_answers = add_cached_column_from_file(\n",
    "    df_questions, \"rag_response_cache.txt\", \"question\", \"answer\"\n",
    ")\n",
    "df_questions_answers\n",
    "df_questions_answers = add_cached_column_from_file(\n",
    "    df_questions_answers, \"rag_response_cache.txt\", \"question\", \"source_documents\"\n",
    ")\n",
    "df_questions_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df_questions_answers.iterrows():\n",
    "    if row[\"answer\"] is None or pd.isnull(row[\"answer\"]):\n",
    "        response = rag_chain.invoke(row[\"question\"])\n",
    "\n",
    "        df_questions_answers.loc[df_questions_answers.index[i], \"answer\"] = response[\n",
    "            \"answer\"\n",
    "        ]\n",
    "        df_questions_answers.loc[df_questions_answers.index[i], \"source_documents\"] = [\n",
    "            stable_hash_meta(source_document.metadata)\n",
    "            for source_document in response[\"source_documents\"]\n",
    "        ]\n",
    "\n",
    "        # optionally save the response to cache\n",
    "        response_dict = {\n",
    "            \"question\": row[\"question\"],\n",
    "            \"answer\": response[\"answer\"],\n",
    "            \"source_documents\": [\n",
    "                stable_hash_meta(source_document.metadata)\n",
    "                for source_document in response[\"source_documents\"]\n",
    "            ],\n",
    "        }\n",
    "        write_dict_to_file(response_dict, \"rag_response_cache.txt\")\n",
    "\n",
    "# get the context documents content for each question\n",
    "df_questions_answers[\"contexts\"] = df_questions_answers[\"source_documents\"].apply(\n",
    "    lambda doc: [df_docs[df_docs[\"id\"] == i][\"document\"].values[0] for i in doc]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# addtionaly get embeddings for questions\n",
    "\n",
    "if not Path(\"question_embeddings_202402201312.pickle\").exists():\n",
    "    question_embeddings = [\n",
    "        embeddings_model.embed_query(question)\n",
    "        for question in df_questions_answers[\"question\"]\n",
    "    ]\n",
    "    with open(\"question_embeddings_202402201312.pickle\", \"wb\") as f:\n",
    "        pickle.dump(question_embeddings, f)\n",
    "\n",
    "question_embeddings = pickle.load(open(\"question_embeddings_202402201312.pickle\", \"rb\"))\n",
    "# answer_embeddings = pickle.load(open(\"answer_embeddings_2040214_1111.pickle\", \"rb\"))\n",
    "df_questions_answers[\"embedding\"] = question_embeddings\n",
    "df_questions_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "In this section we add a new column to the dataframe:\n",
    "- `answer_correctness`: the correctness of the answer of the rag_chain evaluated by ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_vectorstore.get(include=[\"metadatas\", \"documents\", \"embeddings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions_answers = add_cached_column_from_file(\n",
    "    df_questions_answers, \"ragas_result_cache.txt\", \"question\", \"answer_correctness\"\n",
    ")\n",
    "df_questions_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataframe for evaluation\n",
    "df_qa_eval = df_questions_answers.copy()\n",
    "\n",
    "\n",
    "# adapt the ground truth to the ragas name and format\n",
    "df_qa_eval.rename(columns={\"ground_truth\": \"ground_truths\"}, inplace=True)\n",
    "df_qa_eval[\"ground_truths\"] = [\n",
    "    [gt] if not isinstance(gt, list) else gt for gt in df_qa_eval[\"ground_truths\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_correctness\n",
    "from datasets import Dataset\n",
    "\n",
    "# evaluate the answer correctness if not already done\n",
    "fields = [\"question\", \"answer\", \"contexts\", \"ground_truths\"]\n",
    "for i, row in df_qa_eval.iterrows():\n",
    "    if row[\"answer_correctness\"] is None or pd.isnull(row[\"answer_correctness\"]):\n",
    "        evaluation_result = evaluate(\n",
    "            Dataset.from_pandas(df_qa_eval.iloc[i : i + 1][fields]),\n",
    "            [answer_correctness],\n",
    "        )\n",
    "        df_qa_eval.loc[i, \"answer_correctness\"] = evaluation_result[\n",
    "            \"answer_correctness\"\n",
    "        ]\n",
    "\n",
    "        # optionally save the response to cache\n",
    "        response_dict = {\n",
    "            \"question\": row[\"question\"],\n",
    "            \"answer_correctness\": evaluation_result[\"answer_correctness\"],\n",
    "        }\n",
    "        write_dict_to_file(response_dict, \"ragas_result_cache.txt\")\n",
    "\n",
    "# write the answer correctness to the original dataframe\n",
    "df_questions_answers[\"answer_correctness\"] = df_qa_eval[\"answer_correctness\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link from documents to questions, that used the document as source\n",
    "This section adds a column to `df_documents` containing the ids of the questions that used the document as source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the infos about questions using each document to the documents dataframe\n",
    "\n",
    "\n",
    "# Explode 'source_documents' so each document ID is in its own row alongside the question ID\n",
    "df_questions_exploded = df_qa_eval.explode(\"source_documents\")\n",
    "\n",
    "# Group by exploded 'source_documents' (document IDs) and aggregate\n",
    "agg = (\n",
    "    df_questions_exploded.groupby(\"source_documents\")\n",
    "    .agg(\n",
    "        num_questions=(\"id\", \"count\"),  # Count of questions referencing the document\n",
    "        question_ids=(\n",
    "            \"id\",\n",
    "            lambda x: list(x),\n",
    "        ),  # List of question IDs referencing the document\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={\"source_documents\": \"id\"})\n",
    ")\n",
    "\n",
    "# Merge the aggregated information back into df_documents\n",
    "df_documents_agg = pd.merge(df_docs, agg, on=\"id\", how=\"left\")\n",
    "\n",
    "# Use apply to replace NaN values with empty lists for 'question_ids'\n",
    "df_documents_agg[\"question_ids\"] = df_documents_agg[\"question_ids\"].apply(\n",
    "    lambda x: x if isinstance(x, list) else []\n",
    ")\n",
    "# Replace NaN values in 'num_questions' with 0\n",
    "df_documents_agg[\"num_questions\"] = df_documents_agg[\"num_questions\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_qa_eval, df_documents_agg], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create UMAP only using documents and apply it to the documents and questions\n",
    "from umap import UMAP\n",
    "import numpy as np\n",
    "\n",
    "df_questions = df[~df[\"question\"].isna()]\n",
    "umap = UMAP(n_neighbors=20, min_dist=0.15, metric=\"cosine\", random_state=42).fit(\n",
    "    df_questions[\"embedding\"].values.tolist()\n",
    ")\n",
    "umap_questions = umap.transform(df[\"embedding\"].values.tolist())\n",
    "\n",
    "\n",
    "df_without_questions = df[df[\"question\"].isna()]\n",
    "umap = UMAP(n_neighbors=20, min_dist=0.15, metric=\"cosine\", random_state=42).fit(\n",
    "    df_without_questions[\"embedding\"].values.tolist()\n",
    ")\n",
    "umap_docs = umap.transform(df[\"embedding\"].values.tolist())\n",
    "df[\"umap_docs\"] = umap_docs.tolist()\n",
    "\n",
    "umap = UMAP(n_neighbors=20, min_dist=0.15, metric=\"cosine\", random_state=42).fit(\n",
    "    df[\"embedding\"].values.tolist()\n",
    ")\n",
    "umap_all = umap.transform(df[\"embedding\"].values.tolist())\n",
    "df[\"umap\"] = umap_all.tolist()\n",
    "\n",
    "\n",
    "# find the nearet question (by embedding) for each document\n",
    "question_embeddings = np.array(df[df[\"question\"].notna()][\"embedding\"].tolist())\n",
    "\n",
    "df[\"nearest_question_dist\"] = [  # brute force, could be optimized using ChromaDB\n",
    "    np.min([np.linalg.norm(np.array(doc_emb) - question_embeddings, axis=1)])\n",
    "    for doc_emb in df[\"embedding\"].values\n",
    "]\n",
    "\n",
    "# write the dataframe to parquet for later use\n",
    "df.to_parquet(\"df_f1_rag_docs_and_questions_umaps_local.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize\n",
    "Adapt the first cell to use the downloaded dataframes if you skipped the preparation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the df containing the questions and the df containing the documents\n",
    "import pandas as pd\n",
    "\n",
    "# df = pd.concat([df_qa_eval, df_documents_agg], axis=0)\n",
    "\n",
    "# OR Load the data from downloaded file https://spotlightpublic.blob.core.windows.net/docs-data/rag_demo/df_f1_rag_docs_and_questions_umaps_v3.parquet\n",
    "df = pd.read_parquet(\"df_f1_rag_docs_and_questions_umaps_v3.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the dataframe with the question and answer in spotlight\n",
    "from renumics import spotlight\n",
    "from renumics.spotlight import Embedding\n",
    "import pandas as pd\n",
    "\n",
    "spotlight.show(\n",
    "    df,\n",
    "    layout=\"https://spotlightpublic.blob.core.windows.net/docs-data/rag_demo/layout_rag_3.json\",\n",
    "    dtype={x: Embedding for x in df.keys() if \"umap\" in x},\n",
    ")\n",
    "\n",
    "##  UMAP visualization froms cluster of the questions, workaround: UMAP only on documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-demo-IMu3vKF7-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
